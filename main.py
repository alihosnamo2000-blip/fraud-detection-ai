# main.py

import os
import pandas as pd
import joblib
from src.data.preprocess import load_data, clean_data, split_features, split_data
from src.models.classifier import train_model, evaluate_model, save_model
from src.utils.metrics import compute_metrics, compute_auc

def main():
    # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    data_path = 'data/creditcard.csv'
    df = load_data(data_path)

    # 2. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df_clean = clean_data(df)

    # 3. ÙØµÙ„ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¹Ù† Ø§Ù„Ù‡Ø¯Ù
    X, y = split_features(df_clean, target_column='Class')

    # 4. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±
    X_train, X_test, y_train, y_test = split_data(X, y)

    # âœ… Ø·Ø¨Ø§Ø¹Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªÙˆØ§Ø²Ù†
    print(f"ğŸ”¢ Ù‚Ø¨Ù„ Ø§Ù„ØªÙˆØ§Ø²Ù†: Ø¹Ø¯Ø¯ Ø¹ÙŠÙ†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ = {len(X_train)}")
    print(f" - Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª ØºÙŠØ± Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ÙŠØ©: {(y_train == 0).sum()}")
    print(f" - Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ÙŠØ©: {(y_train == 1).sum()}")

    # 5. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªÙˆØ§Ø²Ù† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… undersampling
    train_df = X_train.copy()
    train_df['Class'] = y_train
    fraud = train_df[train_df['Class'] == 1]
    non_fraud = train_df[train_df['Class'] == 0].sample(n=len(fraud), random_state=42)
    balanced_df = pd.concat([fraud, non_fraud])

    X_train = balanced_df.drop(columns=['Class'])
    y_train = balanced_df['Class']

    # âœ… Ø·Ø¨Ø§Ø¹Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆØ§Ø²Ù†
    print(f"\nâš–ï¸ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆØ§Ø²Ù†: Ø¹Ø¯Ø¯ Ø¹ÙŠÙ†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ = {len(X_train)}")
    print(f" - Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª ØºÙŠØ± Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ÙŠØ©: {(y_train == 0).sum()}")
    print(f" - Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ÙŠØ©: {(y_train == 1).sum()}")

    # 6. ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = train_model(X_train, y_train)

    # 7. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    y_pred = model.predict(X_test)
    y_scores = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else y_pred

    metrics = compute_metrics(y_test, y_pred)
    auc = compute_auc(y_test, y_scores)

    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„Ø·Ø±ÙÙŠØ©
    print("\nğŸ“Š Evaluation Metrics:")
    for key, value in metrics.items():
        if key == "Confusion Matrix":
            print(f"{key}:\n{value}")
        else:
            print(f"{key}: {value:.4f}")
    print(f"AUC Score: {auc:.4f}")

    # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ Ù…Ù„Ù Ù†ØµÙŠ
    report_path = 'models/evaluation_report.txt'
    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("ğŸ“Š Evaluation Metrics\n\n")
        for key, value in metrics.items():
            if key == "Confusion Matrix":

                f.write(f"{key}:\n{value}\n\n")
            else:
                f.write(f"{key}: {value:.4f}\n")
        f.write(f"AUC Score: {auc:.4f}\n")

    # 8. Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    save_model(model)
    joblib.dump(model, 'models/random_forest_model.joblib')
    print("\nâœ… All steps completed successfully.")

if __name__ == "__main__":
    main()
